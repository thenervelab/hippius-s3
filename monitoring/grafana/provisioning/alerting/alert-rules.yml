apiVersion: 1

groups:
  - orgId: 1
    name: hippius-s3-critical
    folder: Hippius S3 Alerts
    interval: 1m
    rules:
      - uid: redis-memory-exhaustion
        title: Redis Memory Exhaustion
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: PBFA97CFB590B2093
            model:
              expr: '(redis_memory_used_bytes / redis_memory_max_bytes) * 100'
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    params:
                      - 90
                    type: gt
                  type: query
              refId: C
        noDataState: OK
        execErrState: Error
        for: 3m
        annotations:
          description: 'Redis memory usage is {{ if $values.B }}{{ printf "%.2f" $values.B.Value }}{{ else }}N/A{{ end }}% (threshold: 90%)'
          summary: Redis memory usage critically high
        labels:
          severity: critical

      - uid: queue-backup-critical
        title: Queue Backup Critical
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: PBFA97CFB590B2093
            model:
              expr: 'max(hippius_queue_length{queue_name=~"upload_requests|substrate_requests"})'
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    params:
                      - 1000
                    type: gt
                  type: query
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          description: 'Queue has {{ if $values.B }}{{ printf "%.0f" $values.B.Value }}{{ else }}N/A{{ end }} items (threshold: 1000)'
          summary: Upload or substrate queue is backing up
        labels:
          severity: critical

      - uid: monitoring-pipeline-down
        title: Monitoring Pipeline Down
        condition: A
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: PBFA97CFB590B2093
            model:
              expr: 'absent(up{job="otel-collector"})'
              refId: A
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          description: 'No metrics received from otel-collector for 5+ minutes'
          summary: Monitoring pipeline is down - cannot observe system health
        labels:
          severity: critical

      - uid: consecutive-upload-failures
        title: Consecutive Upload Failures
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: PBFA97CFB590B2093
            model:
              expr: 'increase(s3_errors_total{job="otel-collector",operation="put_object"}[5m])'
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    params:
                      - 3
                    type: gt
                  type: query
              refId: C
        noDataState: OK
        execErrState: Error
        for: 2m
        annotations:
          description: '{{ if $values.B }}{{ printf "%.0f" $values.B.Value }}{{ else }}N/A{{ end }} upload failures in last 5 minutes'
          summary: Multiple upload failures detected - users impacted
        labels:
          severity: critical

  - orgId: 1
    name: hippius-s3-high-priority
    folder: Hippius S3 Alerts
    interval: 1m
    rules:
      - uid: high-error-rate-during-activity
        title: High Error Rate During Activity
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: PBFA97CFB590B2093
            model:
              expr: '((sum(increase(http_requests_total{job="otel-collector"}[5m])) or vector(0)) >= 5) and ((sum(increase(s3_errors_total{job="otel-collector"}[5m])) or vector(0)) / (sum(increase(http_requests_total{job="otel-collector"}[5m])) + 0.0001) * 100)'
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    params:
                      - 15
                    type: gt
                  type: query
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          description: 'Error rate is {{ if $values.B }}{{ printf "%.2f" $values.B.Value }}{{ else }}N/A{{ end }}% during active traffic (threshold: 15%)'
          summary: S3 API experiencing elevated errors during user activity
        labels:
          severity: high

      - uid: slow-response-time-during-activity
        title: Slow Response Time During Activity
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: PBFA97CFB590B2093
            model:
              expr: '((sum(increase(http_requests_total{job="otel-collector"}[5m])) or vector(0)) >= 5) and (histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="otel-collector"}[5m])))'
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: B
              conditions:
                - evaluator:
                    params:
                      - 30
                    type: gt
                  type: query
              refId: C
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          description: 'P95 response time is {{ if $values.B }}{{ printf "%.2f" $values.B.Value }}{{ else }}N/A{{ end }}s during active traffic (threshold: 30s)'
          summary: API response times severely degraded during user activity
        labels:
          severity: high
